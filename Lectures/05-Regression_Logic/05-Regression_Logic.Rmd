---
title: "Regression Logic"
subtitle: "EC 320: Introduction to Econometrics"
date: "Winter 2022"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
# devtools::install_github("dill/emoGG")
library(pacman)
p_load(
  broom, here, tidyverse,
  emoGG, latex2exp, ggplot2, ggthemes, viridis, extrafont, gridExtra,
  kableExtra,
  data.table, dagitty, ggdag,
  dplyr, gganimate,
  lubridate,
  magrittr, knitr, parallel, 
  Ecdat, wooldridge, dslabs, ggforce
)
# Define colors
red_pink <- "#e64173"
met_slate <- "#23373b" # metropolis font color
# Notes directory
dir_slides <- "~/GitHub/EC320_Econometrics/Lectures/05-Regression_Logic/"
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  #dpi = 300,
  #cache = T,
  warning = F,
  message = F
)  
theme_simple <- theme_bw() + theme(
  axis.line = element_line(color = met_slate),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  text = element_text(family = "Fira Sans", color = met_slate, size = 14),
  axis.text.x = element_text(size = 12),
  axis.text.y = element_text(size = 12),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  legend.position = "none"
)
theme_gif <- theme_bw() + theme(
  axis.line = element_line(color = met_slate),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  text = element_text(family = "Fira Sans", color = met_slate, size = 14),
  axis.text.x = element_text(size = 12),
  axis.text.y = element_text(size = 12),
  axis.ticks = element_blank()
)
```

# Prologue

---
# Housekeeping

Exercise 3 due this Wednesday!

Problem Set 1 solution available.

Problem Sets due dates changed
   - Extra two days
   - Due Monday instead of Friday starting Problem Set 2K

Midterm 1 next week (Wednesday)

Midterm review on Monday

---
# Last Time

1. Fundamental problem of econometrics

2. Selection bias

3. Randomized control trials

---
class: inverse, middle

# Regression Logic

---
# Regression

Economists often rely on (linear) regression for statistical comparisons.

- *"Linear"* is more flexible than you think.

Regression analysis helps us make *other things equal* comparisons.

- We can model the effect of $X$ on $Y$ while .hi[controlling] .pink[for potential confounders].
- Forces us to be explicit about the potential sources of selection bias.
- Failure to control for confounding variables leads to .hi[omitted-variable bias], a close cousin of selection bias

---
# Returns to Private College

**Research Question:** Does going to a private college instead of a public college increase future earnings?

- **Outcome variable:** earnings
- **Treatment variable:** going to a private college (binary)

--

**Q:** How might a private school education increase earnings?

--

**Q:** Does a comparison of the average earnings of private college graduates with those of public school graduates .pink[isolate the economic returns to private college education]? Why or why not?

---
# Returns to Private College

**How might we estimate the causal effect of private college on earnings?**

**Approach 1:** Compare average earnings of private college graduates with those of public college graduates.

- Prone to selection bias.

**Approach 2:** Use a matching estimator that compares the earnings of individuals the same admissions profiles.

- Cleaner comparison than a simple difference-in-means.
- Somewhat difficult to implement.
- Throws away data (inefficient).

**Approach 3:** Estimate a regression that compares the earnings of individuals with the same admissions profiles.

<!-- --- -->
<!-- # Difference-in-Means, Take 2 -->

<!-- ## Example: Returns to private college -->

<!-- show same data with groupings based on application profile; what are the differences/similarities within/across groups?; calculate within-group diff-in-means; take average of these (unweighted, then weighted); show and discuss causal diagram -->

<!-- --- -->
<!-- # Difference-in-Means, Regression style -->

<!-- ## Example: Returns to private college -->

<!-- write pop model, describe coefficients and regression lingo; hand wave about OLS and estimated pop model; run regression of example data -->

---
# The Regression Model

We can estimate the effect of $X$ on $Y$ by estimating a .hi[regression model]:

$$Y_i = \beta_0 + \beta_1 X_i + u_i$$

- $Y_i$ is the outcome variable.

--

- $X_i$ is the treatment variable (continuous).

--

- $u_i$ is an error term that includes all other (omitted) factors affecting $Y_i$.

--

- $\beta_0$ is the **intercept** parameter.

--

- $\beta_1$ is the **slope** parameter.

---
# Running Regressions

The intercept and slope are population parameters.

Using an estimator with data on $X_i$ and $Y_i$, we can estimate a .hi[fitted regression line]:

$$\hat{Y_i} = \hat{\beta}_0 + \hat{\beta}_1 X_i$$

- $\hat{Y_i}$ is the **fitted value** of $Y_i$.

- $\hat{\beta}_0$ is the **estimated intercept**.

- $\hat{\beta}_1$ is the **estimated slope**.

--

The estimation procedure produces misses called .hi[residuals], defined as $Y_i - \hat{Y_i}$.

---
# Running Regressions

In practice, we estimate the regression coefficients using an estimator called .hi[Ordinary Least Squares] (OLS).

- Picks estimates that make $\hat{Y_i}$ as close as possible to $Y_i$ given the information we have on $X$ and $Y$.
 
- We will dive into the weeds after the midterm.

---
# Running Regressions

OLS picks $\hat{\beta}_0$ and $\hat{\beta}_1$ that trace out the line of best fit. Ideally, we wound like to interpret the slope of the line as the causal effect of $X$ on $Y$.

```{r, cache = T, include = F}
df <- data.frame(W = as.integer((1:200>100))) %>%
  mutate(X = .5+2*W + rnorm(200)) %>%
  mutate(Y = -.5*X + 4*W + 1 + rnorm(200),time="1") %>%
  group_by(W) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%
  ungroup()
#df %>% lm(Y~X+W, data = .)
```

```{r, dev = "svg", echo = F, fig.height = 5.5}
ggplot(data = df, aes(y = Y, x = X)) +
  geom_point() +
  geom_smooth(method = lm, se = F, color = "#9370DB") +
  theme_simple
```

---
# Confounders

However, the data are grouped by a third variable $W$. How would omitting $W$ from the regression model affect the slope estimator?

```{r, dev = "svg", echo = F, fig.height = 5.5}
ggplot(data = df, aes(y = Y, x = X, color = as.factor(W))) +
  geom_point() +
  theme_gif + 
  labs(color = "W") +
  scale_color_manual(values = c("darkslategrey", red_pink))
```

---
# Confounders

The problem with $W$ is that it affects both $Y$ and $X$. Without adjusting for $W$, we cannot isolate the causal effect of $X$ on $Y$.

```{r, dev = "svg", echo = F, fig.height = 5.5}
dag1 <- dagify(Y ~ X + W,
               X ~ W,
               exposure = "X",
               outcome = "Y")

ggplot(data = dag1, aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point(color = red_pink) +
      geom_dag_edges() +
      geom_dag_text(family = "Fira Sans") +
      theme_dag()
```

---
# Controlling for Confounders

We can control for $W$ by specifying it in the regression model:

$$Y_i = \beta_0 + \beta_1 X_i + \beta_2 W_i + u_i$$

- $W_i$ is a **control variable**.

- By including $W_i$ in the regression, we can use OLS can difference out the confounding effect of $W$.

- **Note:** OLS doesn't care whether a right-hand side variable is a treatment or control variable, but we do.

---
# Controlling for Confounders

```{r, include = F, cache = T}
# the code below produces the gif, but throws an error that prevents me from compiling the slides

# #Calculate correlations
# before_cor <- paste("1. Start with raw data. Correlation between X and Y: ",round(cor(df$X,df$Y),3),sep='')
# after_cor <- paste("6. Analyze what's left! Correlation between X and Y controlling for W: ",round(cor(df$X-df$mean_X,df$Y-df$mean_Y),3),sep='')
# 
# #Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
# dffull <- rbind(
#   #Step 1: Raw data only
#   df %>% mutate(mean_X=NA,mean_Y=NA,time=before_cor),
#   #Step 2: Add x-lines
#   df %>% mutate(mean_Y=NA,time='2. Figure out what differences in X are explained by W'),
#   #Step 3: X de-meaned 
#   df %>% mutate(X = X - mean_X,mean_X=0,mean_Y=NA,time="3. Remove differences in X explained by W"),
#   #Step 4: Remove X lines, add Y
#   df %>% mutate(X = X - mean_X,mean_X=NA,time="4. Figure out what differences in Y are explained by W"),
#   #Step 5: Y de-meaned
#   df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=0,time="5. Remove differences in Y explained by W"),
#   #Step 6: Raw demeaned data only
#   df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=NA,time=after_cor))
# 
# p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(W)))+geom_point()+
#   geom_vline(aes(xintercept=mean_X,color=as.factor(W)))+
#   geom_hline(aes(yintercept=mean_Y,color=as.factor(W)))+
#   guides(color=guide_legend(title="W"))+
#   scale_color_manual(values = c("darkslategrey", red_pink)) +
#   labs(title = 'The Relationship between Y and X, Controlling for a Binary Variable W \n{next_state}')+
#   transition_states(time,transition_length=c(12,32,12,32,12,12),state_length=c(160,100,75,100,75,160),wrap=FALSE)+
#   ease_aes('sine-in-out')+
#   exit_fade()+enter_fade() +
#   theme_gif
# 
# anim_save(
#   animation = p,
#   filename = "control.gif",
#   path = dir_slides,
#   width = 10.5,
#   height = 7,
#   units = "in",
#   res = 150,
#   nframes = 200
# )
```

.center[![Control](control.gif)]

---
# Controlling for Confounders

Controlling for $W$ "adjusts" the data by **differencing out** the group-specific means of $X$ and $Y$. .hi-purple[Slope of the estimated regression line changes!]

```{r, dev = "svg", echo = F, fig.height = 5.5}
df %>%
  mutate(W = as.factor(W)) %>% 
  group_by(W) %>%
  mutate(X = X - mean_X,
         Y = Y - mean_Y) %>%
  ggplot(aes(y = Y, x = X, color = W)) +
  geom_point() +
  geom_smooth(method = lm, se = F, color = "#9370DB") +
  theme_simple +
  xlab("Adjusted X") + ylab("Adjusted Y") +
  scale_color_manual(values = c("darkslategrey", red_pink))
```

---
# Controlling for Confounders

Can we interpret the estimated slope parameter as the causal effect of $X$ on $Y$ now that we've adjusted for $W$?

```{r, dev = "svg", echo = F, fig.height = 5.5}
dag2 <- dagify(Y ~ X + W,
               X ~ W,
               exposure = "X",
               outcome = "Y") 
dag2 %>% 
  node_descendants("W") %>%
  ggplot(aes(
  x = x,
  y = y,
  xend = xend,
  yend = yend,
  color = descendant
  )) +
  geom_dag_point() +
  geom_dag_edges() +
  geom_dag_text(col = "white", family = "Fira Sans") +
  theme_dag() +
  scale_color_hue(breaks  = c("ancestor", "descendant")) +
  theme(legend.position = "none") +
  scale_color_manual(values = c("darkslategrey", red_pink))
```

---
# Controlling for Confounders

## Example: Returns to schooling

Last class:

> **Q:** Could we simply compare the earnings those with more education to those with less?
> <br> **A:** If we want to measure the causal effect, probably not.

.hi-green[What omitted variables should we worry about?]

---
# Controlling for Confounders

## Example: Returns to schooling

Three regressions ***of*** wages ***on*** schooling.

```{R, table, echo = F, escape = F}
tab <- data.frame(
  v1 = c("Intercept", "", "Education", "", "IQ Score", "", "South", ""),
  v2 = rbind(
    c(5.571, 0.052, "", ""),
    c("(0.039)", "(0.003)", "", "")
  ) %>% as.vector(),
  v3 = rbind(
    c(5.581, 0.026, 0.004, ""),
    c("(0.066)", "(0.005)", "(0.001)", "")
  ) %>% as.vector(),
  v4 = rbind(
    c(5.695, 0.027, 0.003, -0.127),
    c("(0.068)", "(0.005)", "(0.001)", "(0.019)")
  ) %>% as.vector()
) %>% kable(
  escape = F,
  col.names = c("Explanatory variable", "1", "2", "3"),
  align = c("l", rep("c", 4)),
  caption = "Outcome variable: log(Wage)"
) %>%
row_spec(1:8, color = met_slate) %>%
row_spec(seq(2,8,2), color = "#c2bebe") %>%
row_spec(1:8, extra_css = "line-height: 110%;") %>%
column_spec(1, color = "black", italic = T)
tab %>% column_spec(2, bold = T)
```

---
count: false

# Controlling for Confounders

## Example: Returns to schooling

Three regressions ***of*** wages ***on*** schooling.

```{R, table2, echo = F}
tab %>% column_spec(3, bold = T)
```

---
count: false

# Controlling for Confounders

## Example: Returns to schooling

Three regressions ***of*** wages ***on*** schooling.

```{R, table3, echo = F}
tab %>% column_spec(4, bold = T)
```

---

# Omitted-Variable Bias

The presence of omitted-variable bias (OVB) precludes causal interpretation of our slope estimates.

We can back out the sign and magnitude of OVB by subtracting the .pink[slope estimate from a ***long*** regression] from the .purple[slope estimate from a ***short*** regression]:

$$\text{OVB} = \color{#9370DB}{\hat{\beta}_1^{\text{Short}}} - \color{#e64173}{\hat{\beta}_1^{\text{Long}}}$$

--

__Dealing with potential sources of OVB is one of the main objectives of econometric analysis!__

<!-- Find example RCT data and run through R example w/ diff-in-means and regression -->

<!-- https://www.povertyactionlab.org/evaluation/summer-jobs-reduce-violence-among-disadvantaged-youth-united-states -->

---
class: inverse, middle

# Data and the .mono[tidyverse]

---
# Data

## Experimental data

Data generated in controlled, laboratory settings.

--

Ideal for __causal identification__, but difficult to obtain in the social sciences.

- Intractable logistical problems
- Too expensive
- Morally repugnant

--

Experiments outside the lab: __randomized control trials__ and __A/B testing__.

---
# Data

## Observational data

Data generated in non-experimental settings.

--

- Surveys
- Censuses
- Administrative records
- Environmental data
- Financial and sales transactions
- Social media

--

Mainstay of economic research, but __poses challenges__ to causal identification.

---
# Tidy Data

.more-left[

```{r, echo=FALSE}
data(murders)
murders <- select(murders, state, population, total)
DT::datatable(
  murders,
  colnames = c('<span style="color: #007935 !important">State</span>', '<span style="color: #007935 !important">Population</span>', '<span style="color: #007935 !important">Murders</span>'),
  fillContainer = FALSE, options = list(pageLength = 6, lengthChange = FALSE, pagingType = "simple"), escape = FALSE) %>%
  DT::formatStyle('state', color = '#9370DB') %>%
  DT::formatStyle('population', color = '#9370DB') %>%
  DT::formatStyle('total', color = '#9370DB') %>%
  DT::formatStyle(0, color = '#FD5F00')
```

]

.less-right[

.hi-orange[Rows] represent .hi-orange[observations].

.hi-green[Columns] represent .hi-green[variables].

Each .hi-purple[value] is associated with an .hi-orange[observation] and a .hi-green[variable].

]

---
# Cross Sectional Data

.hi-purple[Sample of individuals from a population at a point in time.]

Ideally, collected using __random sampling__.

- Random sampling .mono[+] sufficient sample size .mono[=] representative sample.

- Random sampling simplifies data analysis, but non-random samples are common (and difficult to work with).

Used extensively in applied microeconomics.<sup>*</sup>

__Main focus of this course.__

.footnote[
<sup>*</sup> Applied microeconomics .mono[=] Labor, health, education, public finance, development, industrial organization, and urban economics.
]

---
# Cross Sectional Data

```{r, echo=FALSE}
data(wage1)
wage1 <- select(wage1, wage, educ, tenure, female, nonwhite) %>%
  mutate(wage = round(wage, 2))
DT::datatable(
  wage1,
  caption = c("Sample of US workers (Current Population Survey, 1976)"),
  colnames = c('<span style="color: #007935 !important">Wage</span>', '<span style="color: #007935 !important">Education</span>', '<span style="color: #007935 !important">Tenure</span>', '<span style="color: #007935 !important">Female?</span>', '<span style="color: #007935 !important">Non-white?</span>'),
  fillContainer = FALSE, options = list(pageLength = 6, lengthChange = FALSE, searching = FALSE), escape = FALSE) %>%
  DT::formatStyle('wage', color = '#9370DB') %>%
  DT::formatStyle('educ', color = '#9370DB') %>%
  DT::formatStyle('tenure', color = '#9370DB') %>%
  DT::formatStyle('female', color = '#9370DB') %>%
  DT::formatStyle('nonwhite', color = '#9370DB') %>%
  DT::formatStyle(0, color = '#FD5F00')
```

---
# Time Series Data

.hi-purple[Observations of variables over time.]

- Quarterly US GDP
- Annual US infant mortality rates
- Daily Amazon stock prices

Complication: Observations are not independent draws.

- GDP this quarter highly related to GDP last quarter.

Used extensively in empirical macroeconomics.

Requires more-advanced methods (EC 421 and EC 422).

---
# Time Series Data

```{r, echo=FALSE}
data(StrikeNb)
StrikeNb <- select(StrikeNb, time, strikes, output)
DT::datatable(
  StrikeNb,
  caption = c("Number of US manufacturing strikes per month (Jan. 1968 to Dec. 1976)"),
  colnames = c('<span style="color: #007935 !important">Period</span>', '<span style="color: #007935 !important">Strikes</span>', '<span style="color: #007935 !important">Output</span>'),
  fillContainer = FALSE, options = list(pageLength = 6, lengthChange = FALSE, searching = FALSE), escape = FALSE) %>%
  DT::formatStyle('time', color = '#9370DB') %>%
  DT::formatStyle('strikes', color = '#9370DB') %>%
  DT::formatStyle('output', color = '#9370DB') %>%
  DT::formatStyle(0, color = '#FD5F00')
```

---
# Pooled Cross Sectional Data

.hi-purple[Cross sections from different points in time.]

Useful for studying policy changes and relationship that change over time.

Requires more-advanced methods (EC 421 and many 400-level applied micro classes).

---
# Pooled Cross Sectional Data

```{r, echo=FALSE}
data('fertil1')
fertil1 <- select(fertil1, year, educ, age, kids, black)
DT::datatable(
  fertil1,
  caption = c("Sample of US women (General Social Survey, 1972 to 1984)"),
  colnames = c('<span style="color: #007935 !important">Year</span>', '<span style="color: #007935 !important">Education</span>', '<span style="color: #007935 !important">Age</span>', '<span style="color: #007935 !important">Children</span>', '<span style="color: #007935 !important">Black?</span>'),
  fillContainer = FALSE, options = list(pageLength = 6, lengthChange = FALSE, searching = FALSE), escape = FALSE) %>%
  DT::formatStyle('year', color = '#9370DB') %>%
  DT::formatStyle('educ', color = '#9370DB') %>%
  DT::formatStyle('age', color = '#9370DB') %>%
  DT::formatStyle('kids', color = '#9370DB') %>%
  DT::formatStyle('black', color = '#9370DB') %>%
  DT::formatStyle(0, color = '#FD5F00')
```

---
# Panel or Longitudinal Data

.hi-purple[Time series for each cross-sectional unit.]

- Example: daily attendance data for a sample of students.

Difficult to collect, but useful for causal identification.

- Can control for _unobserved_ characteristics.

Requires more-advanced methods (EC 421 and many 400-level applied micro classes).

---
# Panel or Longitudinal Data

```{r, echo=FALSE}
data(Males)
Males <- select(Males, nr, year, exper, wage, union) %>%
  mutate(wage = round(wage, 2))
DT::datatable(
  Males,
  caption = c("Panel of US workers (National Longitudinal Survey of Youth, 1980 to 1987)"),
  colnames = c('<span style="color: #007935 !important">ID</span>', '<span style="color: #007935 !important">Year</span>', '<span style="color: #007935 !important">Experience</span>', '<span style="color: #007935 !important">log(Wage)</span>', '<span style="color: #007935 !important">Union</span>'),
  fillContainer = FALSE, options = list(pageLength = 6, lengthChange = FALSE, searching = FALSE), escape = FALSE) %>%
  DT::formatStyle('nr', color = '#9370DB') %>%
  DT::formatStyle('year', color = '#9370DB') %>%
  DT::formatStyle('exper', color = '#9370DB') %>%
  DT::formatStyle('union', color = '#9370DB') %>%
  DT::formatStyle('wage', color = '#9370DB') %>%
  DT::formatStyle(0, color = '#FD5F00')
```

---
# Tidy Data?

```{r, echo=FALSE}
workers <- read_csv("03-example_data.csv")
DT::datatable(workers, fillContainer = FALSE, options = list(pageLength = 6, lengthChange = FALSE, searching = FALSE), escape = FALSE)
```

---
# Messy Data

**Analysis-ready datasets are rare.** Most data are "messy."

The focus of this class is data analysis, but .hi[data wrangling] is a non-trivial part of a data scientist/analyst's job.

.mono[R] has a suite of packages that facilitate data wrangling. 

- `readr`, `tidyr`, `dplyr`, `ggplot2` .mono[+] others.

- Known collectively as the `tidyverse`.

---
# .mono[tidyverse]

## The [`tidyverse`](https://www.tidyverse.org): A package of packages

`readr`: Functions to import data.

`tidyr`: Functions to reshape messy data.

`dplyr`: Functions to work with data.

`ggplot2`: Functions to visualize data.

---
# Workflow

## Step 1: Load packages with `pacman`

```{r}
library(pacman)
p_load(tidyverse)
```

If the `tidyverse` hasn't already been installed, `p_load` will install it. 

Loading the `tidyverse` automatically loads `readr`, `tidyr`, `dplyr`, `ggplot2`, and a few other packages.

---
# Workflow

## Step 2: Import data with `readr`

```{r}
workers <- read_csv("03-example_data.csv")
```

CSV files are a common non-proprietary format for storing tabular data.

The `read_csv` function imports CSV (comma-separated values) files.

- Converts the CSV file to a [`tibble`](https://tibble.tidyverse.org), the `tidyverse` version of a `data.frame`.

---
# Workflow

## Step 3: Reshape data with `tidyr`

Variables are stored in rows instead of columns:

```{r, echo=FALSE}
workers
```

---
# Workflow

## Step 3: Reshape data with `tidyr`

Make the data tidy by using the `spread` function:

```{r}
workers <- workers %>% 
  spread(key = variable, value = value)
```

Note the use of the .hi[pipe operator].

- .hi[`%>%`] .mono[=] *"and then."*

- Chains multiple commands together without having to define intermediate objects.

---
# Workflow

## Step 3: Reshape data with `tidyr`

The result:

```{r, echo=FALSE}
workers
```

---
# Workflow

## Step 4: Manipulate data with `dplyr`

Generate new variables with `mutate`:

```{r}
workers <- workers %>% 
  mutate(union = ifelse(union == 1, "Yes", "No"))
```

Before, `union` was a binary variable equal to 1 if the worker is in a union or 0 if otherwise.

Now `union` is a character variable.

---
# Workflow

## Step 4: Manipulate data with `dplyr`

The result:

```{r, echo=FALSE}
workers
```

---
# Workflow

## Step 6: Visualize and analyze data with `ggplot2`

**How are education and earnings correlated?**

```{r, fig.height = 3.5}
workers %>% 
  ggplot(aes(x = educ, y = earnings)) +
  geom_point()
```

---
# Workflow

## Step 6: Visualize and analyze data with `ggplot2`

**How are education and earnings correlated?**

Can also use the `cor` function from `base` .mono[R]:

```{r}
cor(workers$educ, workers$earnings)
```

---
# Workflow

## Step 6: Visualize and analyze data with `ggplot2`

**How are education and earnings correlated?**

```{r, fig.height = 3.5}
workers %>% 
  ggplot(aes(x = educ, y = earnings, color = union)) +
  geom_point()
```

---
# Workflow

## Step 6: Visualize and analyze data with `ggplot2`

**How are education and earnings correlated?**

```{r, fig.height = 3.5}
workers %>% 
  ggplot(aes(x = educ, y = earnings, color = union)) +
  geom_point() +
  facet_grid(~union)
```

---
# Workflow

## Step 6: Visualize and analyze data with `ggplot2`

**How are education and earnings correlated?**

Can .hi[subset] the data to get group-specific correlations: 

```{r}
workers_union <- workers %>% 
  filter(union == "Yes") #<<
cor(workers_union$educ, workers_union$earnings)
```

```{r}
workers_nounion <- workers %>% 
  filter(union == "No") #<<
cor(workers_nounion$educ, workers_nounion$earnings)
```

---
# Why Bother?

**Q:** Why not just use .mono[.hi-green[MS Excel]] for data wrangling?

--

**A:** .hi[Reproducibility]

- Easier to retrace your steps with .mono[R].

--

**A:** .hi[Portability]

- Easy to re-purpose .mono[R] code for new projects.

--

**A:** .hi[Scalability]

- .mono[Excel] chokes on big datasets.

--

**A:** .hi[.mono[R] Saves time] (eventually)

- Lower marginal costs in exchange for higher fixed costs.

---
# Further Reading

1. [Tidy Data](https://vita.had.co.nz/papers/tidy-data.pdf) by Hadley Wickham (creator of the `tidyverse`)

2. [Cheatsheets](https://rstudio.com/resources/cheatsheets/)


